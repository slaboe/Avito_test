# Text Space Restoration with BERT

Проект предназначен для восстановления пробелов в текстах. Я пробовал множество вариантов решения: классические NLP-методы, предобученный токенизатор BPE, LTSM, RandomForestClassifier с подбором порога, однако все эти методы показывали низкий f1 на тестовой выборке, выбор пал на BERT. Допускаю что есть способ решения в разы легче с меньшей затратой ресурсов.


---

## Шаги решения

### 1. Подготовка корпуса
- В папке `news_raw/texts` находится заранее загруженные коллекции из корпуса Taiga https://tatianashavrina.github.io/taiga_site/downloads
- Все текстовые файлы из папки `news_raw/texts` собираются в один файл `corpus.txt`.
- HTML-теги (`script`, `style`) удаляются.
- Лишние пробелы и переносы строк нормализуются.
- Результатом является чистый текстовый корпус для дальнейшей работы.

### 2. Разбиение на предложения
- С помощью библиотеки `razdel` тексты разбиваются на предложения.
- Отбрасываются слишком короткие предложения (менее 4 символов).
- Генерируется список предложений для создания датасета.

### 3. Генерация синтетического датасета
- Для каждого предложения сохраняется:
  - исходный текст,
  - текст без пробелов,
  - позиции пробелов.
- Датасет делится на:
  - `train` — обучающая выборка,
  - `val` — валидационная выборка,
  - `test` — тестовая выборка.
- Удаляются пересечения между выборками.
- CSV-файлы сохраняются в папку `data`:
  - `data/train.csv`
  - `data/val.csv`
  - `data/test.csv`

### 4. Фильтрация коротких строк
- Для ускорения обучения отбираются строки длиной до `MAX_LEN/2`.
- Это помогает модели быстрее обучаться на коротких примерах.

### 5. Создание датасетов для PyTorch
- **SpaceDataset**:
  - Принимает тексты без пробелов и позиции пробелов.
  - Возвращает `input_ids`, `attention_mask` и метки (`labels`) для обучения.
- Создаются `DataLoader` для обучения и валидации.

### 6. Инициализация модели и оптимизатора
- Модель: `BertForTokenClassification` с 2 классами (пробел / не пробел).
- Оптимизатор: `AdamW`.
- Функция потерь: `CrossEntropyLoss`.
- Все вычисления выполняются на GPU при наличии (`device = cuda`).

### 7. Обучение модели
- Функции:
  - `train_epoch` — обучение на одной эпохе.
  - `validate` — вычисление потерь и F1 на валидации.
- Для каждой эпохи:
  - Прямой проход через модель.
  - Вычисление градиентов и обновление весов.
  - Сохранение лучшей модели по F1.

### 8. Инференс на новых данных
- Создается **TaskDataset** для любых новых текстов без пробелов.
- Прогон через обученную модель (`model.eval()`).
- Восстановление пробелов по предсказанным позициям.
- Сохраняется в `submission.csv` с колонками:
  - `id`
  - `text_no_spaces`
  - `predicted_positions`
  - `predicted_text`


---

## Установка зависимостей

1. Создайте виртуальное окружение: python -m venv venv
2. Активируйте окружение: .venv\Scripts\activate
3. Установите зависимости pip install -r requirements.txt

---

## Используемые ресурсы GPU

- **Обучение модели**:
    - GPU: NVIDIA RTX 3080 / 12 GB VRAM
    - Обучение на `BATCH_SIZE=16`, `MAX_LEN=128`
    - Время одной эпохи: ~4 минуты

- **Инференс (предсказание на новых данных)**:
    - GPU: можно использовать ту же карту, но допустимо и CPU
    - Время обработки ~0.5–2 секунды на 1000 коротких строк