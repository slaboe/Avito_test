{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6344b713",
   "metadata": {},
   "source": [
    "# Восстановление пробелов в тексте"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be61007",
   "metadata": {},
   "source": [
    "Подготовим импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65516912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from razdel import sentenize\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from torch.optim import AdamW \n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600c0aff",
   "metadata": {},
   "source": [
    "Создание корпуса для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eac99681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus.txt готов! Всего строк: 36445\n"
     ]
    }
   ],
   "source": [
    "input_folder = \"news_raw/texts\"\n",
    "output_file = \"corpus.txt\"\n",
    "\n",
    "all_files = glob.glob(os.path.join(input_folder, \"**/*.txt\"), recursive=True)\n",
    "lines = []\n",
    "\n",
    "for f in all_files:\n",
    "    with open(f, \"r\", encoding=\"utf-8\") as file:\n",
    "        html = file.read()\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # Удаляем script и style\n",
    "        for tag in soup([\"script\", \"style\"]):\n",
    "            tag.decompose()\n",
    "\n",
    "        text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "        # Убираем лишние символы переноса строк\n",
    "        text = text.replace('\\u2028',' ').replace('\\u2029',' ')\n",
    "\n",
    "        # Сжимаем множественные пробелы в один\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        if text:\n",
    "            lines.append(text)\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in lines:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(\"corpus.txt готов! Всего строк:\", len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a6287e",
   "metadata": {},
   "source": [
    "Подготовим корпус к обучению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302ffc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positions_from_text(s):\n",
    "    \"\"\"Возвращает индексы пробелов в строке\"\"\"\n",
    "    return [i+1 for i, c in enumerate(s[:-1]) if s[i+1].isspace()]\n",
    "\n",
    "def remove_spaces(s):\n",
    "    \"\"\"Удаляет пробелы\"\"\"\n",
    "    return s.replace(\" \", \"\")\n",
    "\n",
    "def extract_sentences_from_file(filename):\n",
    "    \"\"\"Читает файл и разбивает на предложения с помощью razdel\"\"\"\n",
    "    sentences = []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            for sent in sentenize(line):\n",
    "                text = sent.text.strip()\n",
    "                if len(text) > 3:  # минимальная длина предложения\n",
    "                    sentences.append(text)\n",
    "    return sentences\n",
    "\n",
    "def generate_synthetic_dataset(sentences, seed=42, val_size=0.1, test_size=0.1):\n",
    "    random.seed(seed)\n",
    "    data = []\n",
    "    for i, line in enumerate(sentences):\n",
    "        line_norm = \" \".join(line.split())  # убираем лишние пробелы\n",
    "        no_space = remove_spaces(line_norm)\n",
    "        positions = positions_from_text(line_norm)\n",
    "        data.append({\n",
    "            \"id\": i,\n",
    "            \"text\": line_norm,\n",
    "            \"no_space\": no_space,\n",
    "            \"space_positions\": json.dumps(positions)\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    # Делим на train/val/test\n",
    "    train_df, temp_df = train_test_split(df, test_size=val_size+test_size, random_state=seed)\n",
    "    val_ratio = val_size / (val_size + test_size)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=1-val_ratio, random_state=seed)\n",
    "\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5fc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Извлекаем предложения...\n",
      "Всего предложений: 293223\n",
      "Генерируем синтетический датасет...\n",
      "Синтетический датасет создан!\n",
      "Train: 234578, Val: 29322, Test: 29323\n"
     ]
    }
   ],
   "source": [
    "corpus_file = \"corpus.txt\"  \n",
    "if not os.path.exists(corpus_file):\n",
    "    raise FileNotFoundError(f\"Файл {corpus_file} не найден!\")\n",
    "\n",
    "print(\"Извлекаем предложения...\")\n",
    "sentences = extract_sentences_from_file(corpus_file)\n",
    "print(f\"Всего предложений: {len(sentences)}\")\n",
    "\n",
    "print(\"Генерируем синтетический датасет...\")\n",
    "train_df, val_df, test_df = generate_synthetic_dataset(sentences)\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "train_df.to_csv(\"data/train.csv\", index=False)\n",
    "val_df.to_csv(\"data/val.csv\", index=False)\n",
    "test_df.to_csv(\"data/test.csv\", index=False)\n",
    "\n",
    "print(\"Синтетический датасет создан!\")\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c9f76e",
   "metadata": {},
   "source": [
    "Так как данные из новостей, они могут пересекаться, проверим и удалим дубликаты во всех датасетах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e9746ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Пересечение train & val:\n",
      "  ID пересекаются: 0\n",
      "  Text пересекаются: 595\n",
      "\n",
      "Пересечение train & test:\n",
      "  ID пересекаются: 0\n",
      "  Text пересекаются: 603\n",
      "\n",
      "Пересечение val & test:\n",
      "  ID пересекаются: 0\n",
      "  Text пересекаются: 130\n"
     ]
    }
   ],
   "source": [
    "def check_dataset_overlap(train_file, val_file, test_file):\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    val_df = pd.read_csv(val_file)\n",
    "    test_df = pd.read_csv(test_file)\n",
    "\n",
    "    datasets = {\"train\": train_df, \"val\": val_df, \"test\": test_df}\n",
    "    pairs = [(\"train\", \"val\"), (\"train\", \"test\"), (\"val\", \"test\")]\n",
    "\n",
    "    for a, b in pairs:\n",
    "        df_a, df_b = datasets[a], datasets[b]\n",
    "\n",
    "        id_overlap = set(df_a['id']).intersection(df_b['id'])\n",
    "        text_overlap = set(df_a['text']).intersection(df_b['text'])\n",
    "\n",
    "        print(f\"\\nПересечение {a} & {b}:\")\n",
    "        print(f\"  ID пересекаются: {len(id_overlap)}\")\n",
    "        print(f\"  Text пересекаются: {len(text_overlap)}\")\n",
    "\n",
    "check_dataset_overlap(\n",
    "    \"data/train.csv\",\n",
    "    \"data/val.csv\",\n",
    "    \"data/test.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3bf17977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пересечения удалены. Новые размеры:\n",
      "Train: 234578, Val: 28628, Test: 28568\n"
     ]
    }
   ],
   "source": [
    "val_df = val_df[~val_df['text'].isin(train_df['text'])]\n",
    "test_df = test_df[~test_df['text'].isin(train_df['text'])]\n",
    "test_df = test_df[~test_df['text'].isin(val_df['text'])]\n",
    "\n",
    "val_df.to_csv(\"data/val.csv\", index=False)\n",
    "test_df.to_csv(\"data/test.csv\", index=False)\n",
    "\n",
    "print(\"Пересечения удалены. Новые размеры:\")\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccd6354",
   "metadata": {},
   "source": [
    "Подготовка к обучению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d6489dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "MAX_LEN = 128    \n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 50\n",
    "\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a4fa413",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "val_df   = pd.read_csv(\"data/val.csv\")\n",
    "test_df  = pd.read_csv(\"data/test.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e1e275f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>no_space</th>\n",
       "      <th>space_positions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8135</th>\n",
       "      <td>94411</td>\n",
       "      <td>Последний из них, \"Dig Out Your Soul\", поступи...</td>\n",
       "      <td>Последнийизних,\"DigOutYourSoul\",поступилвпрода...</td>\n",
       "      <td>[9, 12, 17, 22, 26, 31, 38, 47, 49, 57, 64, 69]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>283878</td>\n",
       "      <td>«Сирию раздирают войны и конфликты.</td>\n",
       "      <td>«Сириюраздираютвойныиконфликты.</td>\n",
       "      <td>[6, 16, 22, 24]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49394</th>\n",
       "      <td>474</td>\n",
       "      <td>Сообщается, что клубы могут заключить сделку в...</td>\n",
       "      <td>Сообщается,чтоклубымогутзаключитьсделкувближай...</td>\n",
       "      <td>[11, 15, 21, 27, 37, 44, 46, 56]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121378</th>\n",
       "      <td>87546</td>\n",
       "      <td>В России работают более 270 закусочных McDonal...</td>\n",
       "      <td>ВРоссииработаютболее270закусочныхMcDonald's.</td>\n",
       "      <td>[1, 8, 17, 23, 27, 38]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38025</th>\n",
       "      <td>7215</td>\n",
       "      <td>Дату проверки источник не указал.</td>\n",
       "      <td>Датупроверкиисточникнеуказал.</td>\n",
       "      <td>[4, 13, 22, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160285</th>\n",
       "      <td>190910</td>\n",
       "      <td>Пророссийские митинги также прошли в других ре...</td>\n",
       "      <td>Пророссийскиемитингитакжепрошливдругихрегионах...</td>\n",
       "      <td>[13, 21, 27, 34, 36, 43, 52, 56, 58, 66]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174591</th>\n",
       "      <td>235977</td>\n",
       "      <td>Миссия осуждает такие действия.</td>\n",
       "      <td>Миссияосуждаеттакиедействия.</td>\n",
       "      <td>[6, 15, 21]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3284</th>\n",
       "      <td>125190</td>\n",
       "      <td>В каком состоянии находится животное и как себ...</td>\n",
       "      <td>Вкакомсостояниинаходитсяживотноеикаксебячувств...</td>\n",
       "      <td>[1, 7, 17, 27, 36, 38, 42, 47, 58, 61]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185799</th>\n",
       "      <td>289613</td>\n",
       "      <td>Спустя пять лет он был переизбран на второй срок.</td>\n",
       "      <td>Спустяпятьлетонбылпереизбраннавторойсрок.</td>\n",
       "      <td>[6, 11, 15, 18, 22, 33, 36, 43]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122675</th>\n",
       "      <td>222433</td>\n",
       "      <td>Всего с платформы эвакуированы около 300 работ...</td>\n",
       "      <td>Всегосплатформыэвакуированыоколо300работников.</td>\n",
       "      <td>[5, 7, 17, 30, 36, 40]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156199</th>\n",
       "      <td>118963</td>\n",
       "      <td>Три месяца спустя вышла версия ОС для разработ...</td>\n",
       "      <td>ТримесяцаспустявышлаверсияОСдляразработчиков.</td>\n",
       "      <td>[3, 10, 17, 23, 30, 33, 37]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60451</th>\n",
       "      <td>203063</td>\n",
       "      <td>В ходе проверки будет выяснено, сдавал ли поги...</td>\n",
       "      <td>Входепроверкибудетвыяснено,сдаваллипогибшийтес...</td>\n",
       "      <td>[1, 6, 15, 21, 31, 38, 41, 50, 55, 58]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72145</th>\n",
       "      <td>291931</td>\n",
       "      <td>У меня было большое желание вернуться в Россию.</td>\n",
       "      <td>УменябылобольшоежеланиевернутьсявРоссию.</td>\n",
       "      <td>[1, 6, 11, 19, 27, 37, 39]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174685</th>\n",
       "      <td>269137</td>\n",
       "      <td>Приземлиться путешественник намерен на восточн...</td>\n",
       "      <td>Приземлитьсяпутешественникнамереннавосточномпо...</td>\n",
       "      <td>[12, 27, 35, 38, 48, 58]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225839</th>\n",
       "      <td>260300</td>\n",
       "      <td>Проект документа совместно подготовили Москва ...</td>\n",
       "      <td>ПроектдокументасовместноподготовилиМоскваиВаши...</td>\n",
       "      <td>[6, 16, 26, 38, 45, 47]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52286</th>\n",
       "      <td>215473</td>\n",
       "      <td>Литва официально перешла на единую европейскую...</td>\n",
       "      <td>Литваофициальноперешланаединуюевропейскуювалюту.</td>\n",
       "      <td>[5, 16, 24, 27, 34, 46]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218266</th>\n",
       "      <td>132974</td>\n",
       "      <td>В первую входят три гена, отвечающие за дыхани...</td>\n",
       "      <td>Впервуювходяттригена,отвечающиезадыханиеввысот...</td>\n",
       "      <td>[1, 8, 15, 19, 25, 36, 39, 47, 49, 58]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123442</th>\n",
       "      <td>271894</td>\n",
       "      <td>«Домик переносной, он такой, что его может сдуть.</td>\n",
       "      <td>«Домикпереносной,онтакой,чтоегоможетсдуть.</td>\n",
       "      <td>[6, 18, 21, 28, 32, 36, 42]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42896</th>\n",
       "      <td>151346</td>\n",
       "      <td>Отзыв на стихи Маркова написала, в частности, ...</td>\n",
       "      <td>ОтзывнастихиМаркованаписала,вчастности,НинаБер...</td>\n",
       "      <td>[5, 8, 14, 22, 32, 34, 45, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80833</th>\n",
       "      <td>10565</td>\n",
       "      <td>Инаугурация новоизбранного президента состоитс...</td>\n",
       "      <td>Инаугурацияновоизбранногопрезидентасостоитсято...</td>\n",
       "      <td>[11, 26, 37, 47, 54, 56]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text  \\\n",
       "8135     94411  Последний из них, \"Dig Out Your Soul\", поступи...   \n",
       "151     283878                «Сирию раздирают войны и конфликты.   \n",
       "49394      474  Сообщается, что клубы могут заключить сделку в...   \n",
       "121378   87546  В России работают более 270 закусочных McDonal...   \n",
       "38025     7215                  Дату проверки источник не указал.   \n",
       "160285  190910  Пророссийские митинги также прошли в других ре...   \n",
       "174591  235977                    Миссия осуждает такие действия.   \n",
       "3284    125190  В каком состоянии находится животное и как себ...   \n",
       "185799  289613  Спустя пять лет он был переизбран на второй срок.   \n",
       "122675  222433  Всего с платформы эвакуированы около 300 работ...   \n",
       "156199  118963  Три месяца спустя вышла версия ОС для разработ...   \n",
       "60451   203063  В ходе проверки будет выяснено, сдавал ли поги...   \n",
       "72145   291931    У меня было большое желание вернуться в Россию.   \n",
       "174685  269137  Приземлиться путешественник намерен на восточн...   \n",
       "225839  260300  Проект документа совместно подготовили Москва ...   \n",
       "52286   215473  Литва официально перешла на единую европейскую...   \n",
       "218266  132974  В первую входят три гена, отвечающие за дыхани...   \n",
       "123442  271894  «Домик переносной, он такой, что его может сдуть.   \n",
       "42896   151346  Отзыв на стихи Маркова написала, в частности, ...   \n",
       "80833    10565  Инаугурация новоизбранного президента состоитс...   \n",
       "\n",
       "                                                 no_space  \\\n",
       "8135    Последнийизних,\"DigOutYourSoul\",поступилвпрода...   \n",
       "151                       «Сириюраздираютвойныиконфликты.   \n",
       "49394   Сообщается,чтоклубымогутзаключитьсделкувближай...   \n",
       "121378       ВРоссииработаютболее270закусочныхMcDonald's.   \n",
       "38025                       Датупроверкиисточникнеуказал.   \n",
       "160285  Пророссийскиемитингитакжепрошливдругихрегионах...   \n",
       "174591                       Миссияосуждаеттакиедействия.   \n",
       "3284    Вкакомсостояниинаходитсяживотноеикаксебячувств...   \n",
       "185799          Спустяпятьлетонбылпереизбраннавторойсрок.   \n",
       "122675     Всегосплатформыэвакуированыоколо300работников.   \n",
       "156199      ТримесяцаспустявышлаверсияОСдляразработчиков.   \n",
       "60451   Входепроверкибудетвыяснено,сдаваллипогибшийтес...   \n",
       "72145            УменябылобольшоежеланиевернутьсявРоссию.   \n",
       "174685  Приземлитьсяпутешественникнамереннавосточномпо...   \n",
       "225839  ПроектдокументасовместноподготовилиМоскваиВаши...   \n",
       "52286    Литваофициальноперешланаединуюевропейскуювалюту.   \n",
       "218266  Впервуювходяттригена,отвечающиезадыханиеввысот...   \n",
       "123442         «Домикпереносной,онтакой,чтоегоможетсдуть.   \n",
       "42896   ОтзывнастихиМаркованаписала,вчастности,НинаБер...   \n",
       "80833   Инаугурацияновоизбранногопрезидентасостоитсято...   \n",
       "\n",
       "                                        space_positions  \n",
       "8135    [9, 12, 17, 22, 26, 31, 38, 47, 49, 57, 64, 69]  \n",
       "151                                     [6, 16, 22, 24]  \n",
       "49394                  [11, 15, 21, 27, 37, 44, 46, 56]  \n",
       "121378                           [1, 8, 17, 23, 27, 38]  \n",
       "38025                                   [4, 13, 22, 25]  \n",
       "160285         [13, 21, 27, 34, 36, 43, 52, 56, 58, 66]  \n",
       "174591                                      [6, 15, 21]  \n",
       "3284             [1, 7, 17, 27, 36, 38, 42, 47, 58, 61]  \n",
       "185799                  [6, 11, 15, 18, 22, 33, 36, 43]  \n",
       "122675                           [5, 7, 17, 30, 36, 40]  \n",
       "156199                      [3, 10, 17, 23, 30, 33, 37]  \n",
       "60451            [1, 6, 15, 21, 31, 38, 41, 50, 55, 58]  \n",
       "72145                        [1, 6, 11, 19, 27, 37, 39]  \n",
       "174685                         [12, 27, 35, 38, 48, 58]  \n",
       "225839                          [6, 16, 26, 38, 45, 47]  \n",
       "52286                           [5, 16, 24, 27, 34, 46]  \n",
       "218266           [1, 8, 15, 19, 25, 36, 39, 47, 49, 58]  \n",
       "123442                      [6, 18, 21, 28, 32, 36, 42]  \n",
       "42896                    [5, 8, 14, 22, 32, 34, 45, 50]  \n",
       "80833                          [11, 26, 37, 47, 54, 56]  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157d49da",
   "metadata": {},
   "source": [
    "Так как в тестовом файле от Авито в основном короткие словосочетания, такие как книгахорошая, оставим из исходного датасета только короткие строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8cc55c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество коротких строк: 36160\n",
      "Количество коротких строк: 4285\n",
      "Количество коротких строк: 4023\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df[train_df['no_space'].str.len() <= MAX_LEN/2].copy()\n",
    "val_df = val_df[val_df['no_space'].str.len() <= MAX_LEN/2].copy()\n",
    "test_df = test_df[test_df['no_space'].str.len() <= MAX_LEN/2].copy()\n",
    "print(f\"Количество коротких строк: {len(train_df)}\")\n",
    "print(f\"Количество коротких строк: {len(val_df)}\")\n",
    "print(f\"Количество коротких строк: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "adfa26e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>no_space</th>\n",
       "      <th>space_positions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>228066</th>\n",
       "      <td>258970</td>\n",
       "      <td>«Наши спортсмены давно не употребляют запрещен...</td>\n",
       "      <td>«Нашиспортсменыдавнонеупотребляютзапрещенныхпр...</td>\n",
       "      <td>[5, 16, 22, 25, 37, 49]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21730</th>\n",
       "      <td>153928</td>\n",
       "      <td>Кроме того, в аварии могли пострадать водитель...</td>\n",
       "      <td>Крометого,вавариимоглипострадатьводительипасса...</td>\n",
       "      <td>[5, 11, 13, 20, 26, 37, 46, 48, 58]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194526</th>\n",
       "      <td>168136</td>\n",
       "      <td>В числе последних назывались «ЮТэйр», «Сибирь»...</td>\n",
       "      <td>Вчислепоследнихназывались«ЮТэйр»,«Сибирь»(S7)и...</td>\n",
       "      <td>[1, 7, 17, 28, 37, 46, 51, 53]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32380</th>\n",
       "      <td>267288</td>\n",
       "      <td>Вместо этого руководство ЦБ сделало все правил...</td>\n",
       "      <td>ВместоэтогоруководствоЦБсделаловсеправильно»,—...</td>\n",
       "      <td>[6, 12, 24, 27, 35, 39, 51, 53, 61]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71417</th>\n",
       "      <td>108723</td>\n",
       "      <td>В 2010 году она отобрала это звание у Microsoft.</td>\n",
       "      <td>В2010годуонаотобралаэтозваниеуMicrosoft.</td>\n",
       "      <td>[1, 6, 11, 15, 24, 28, 35, 37]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216410</th>\n",
       "      <td>57061</td>\n",
       "      <td>29 августа 2010 года группа выступила в москов...</td>\n",
       "      <td>29августа2010годагруппавыступилавмосковскомклу...</td>\n",
       "      <td>[2, 10, 15, 20, 27, 37, 39, 50, 56]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126138</th>\n",
       "      <td>122764</td>\n",
       "      <td>В первое семейство вошли модели с экранами в 1...</td>\n",
       "      <td>Впервоесемействовошлимоделисэкранамив11,6,14и1...</td>\n",
       "      <td>[1, 8, 18, 24, 31, 33, 42, 44, 50, 53, 55, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21128</th>\n",
       "      <td>126568</td>\n",
       "      <td>На первом месте \"Реал\", у которого 78 очков и ...</td>\n",
       "      <td>Напервомместе\"Реал\",укоторого78очковиоднаиграв...</td>\n",
       "      <td>[2, 9, 15, 23, 25, 34, 37, 43, 45, 50, 55, 57]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205125</th>\n",
       "      <td>105518</td>\n",
       "      <td>О том, когда выйдет Dragon Quest X для Wii U, ...</td>\n",
       "      <td>Отом,когдавыйдетDragonQuestXдляWiiU,поканесооб...</td>\n",
       "      <td>[1, 6, 12, 19, 26, 32, 34, 38, 42, 45, 50, 53]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46819</th>\n",
       "      <td>186844</td>\n",
       "      <td>Всего Следственным департаментом МВД задержано...</td>\n",
       "      <td>ВсегоСледственнымдепартаментомМВДзадержано12че...</td>\n",
       "      <td>[5, 18, 32, 36, 46, 49]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170623</th>\n",
       "      <td>167653</td>\n",
       "      <td>На место прибыл наряд полиции.</td>\n",
       "      <td>Наместоприбылнарядполиции.</td>\n",
       "      <td>[2, 8, 15, 21]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119309</th>\n",
       "      <td>139526</td>\n",
       "      <td>Инцидент произошел 26 августа в супермаркете W...</td>\n",
       "      <td>Инцидентпроизошел26августавсупермаркетеWalmart...</td>\n",
       "      <td>[8, 18, 21, 29, 31, 44, 52, 54]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158759</th>\n",
       "      <td>142507</td>\n",
       "      <td>Я люблю тебя!\"</td>\n",
       "      <td>Ялюблютебя!\"</td>\n",
       "      <td>[1, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87631</th>\n",
       "      <td>278476</td>\n",
       "      <td>Мы в какой стране живем?</td>\n",
       "      <td>Мывкакойстранеживем?</td>\n",
       "      <td>[2, 4, 10, 17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32687</th>\n",
       "      <td>120762</td>\n",
       "      <td>Об этом рассказал премьер-министр Турции Редже...</td>\n",
       "      <td>Обэтомрассказалпремьер-министрТурцииРеджепТайи...</td>\n",
       "      <td>[2, 7, 17, 33, 40, 47, 53]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170164</th>\n",
       "      <td>55111</td>\n",
       "      <td>Судмедэкспертиза признала его вменяемым.</td>\n",
       "      <td>Судмедэкспертизапризналаеговменяемым.</td>\n",
       "      <td>[16, 25, 29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213412</th>\n",
       "      <td>152350</td>\n",
       "      <td>Педагога доставили в больницу, где ему сделали...</td>\n",
       "      <td>Педагогадоставиливбольницу,гдеемусделалиоперацию.</td>\n",
       "      <td>[8, 18, 20, 30, 34, 38, 46]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190141</th>\n",
       "      <td>141172</td>\n",
       "      <td>Защита обжаловала приговор в Мосгорсуде.</td>\n",
       "      <td>ЗащитаобжаловалаприговорвМосгорсуде.</td>\n",
       "      <td>[6, 17, 26, 28]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>274912</td>\n",
       "      <td>Самый значительный прирост зафиксирован в Афри...</td>\n",
       "      <td>СамыйзначительныйприростзафиксированвАфрике(пл...</td>\n",
       "      <td>[5, 18, 26, 39, 41, 48, 54, 59]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225877</th>\n",
       "      <td>30846</td>\n",
       "      <td>На борту находились активисты европейских прав...</td>\n",
       "      <td>Набортунаходилисьактивистыевропейскихправозащи...</td>\n",
       "      <td>[2, 8, 19, 29, 41, 55]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text  \\\n",
       "228066  258970  «Наши спортсмены давно не употребляют запрещен...   \n",
       "21730   153928  Кроме того, в аварии могли пострадать водитель...   \n",
       "194526  168136  В числе последних назывались «ЮТэйр», «Сибирь»...   \n",
       "32380   267288  Вместо этого руководство ЦБ сделало все правил...   \n",
       "71417   108723   В 2010 году она отобрала это звание у Microsoft.   \n",
       "216410   57061  29 августа 2010 года группа выступила в москов...   \n",
       "126138  122764  В первое семейство вошли модели с экранами в 1...   \n",
       "21128   126568  На первом месте \"Реал\", у которого 78 очков и ...   \n",
       "205125  105518  О том, когда выйдет Dragon Quest X для Wii U, ...   \n",
       "46819   186844  Всего Следственным департаментом МВД задержано...   \n",
       "170623  167653                     На место прибыл наряд полиции.   \n",
       "119309  139526  Инцидент произошел 26 августа в супермаркете W...   \n",
       "158759  142507                                     Я люблю тебя!\"   \n",
       "87631   278476                           Мы в какой стране живем?   \n",
       "32687   120762  Об этом рассказал премьер-министр Турции Редже...   \n",
       "170164   55111           Судмедэкспертиза признала его вменяемым.   \n",
       "213412  152350  Педагога доставили в больницу, где ему сделали...   \n",
       "190141  141172           Защита обжаловала приговор в Мосгорсуде.   \n",
       "14636   274912  Самый значительный прирост зафиксирован в Афри...   \n",
       "225877   30846  На борту находились активисты европейских прав...   \n",
       "\n",
       "                                                 no_space  \\\n",
       "228066  «Нашиспортсменыдавнонеупотребляютзапрещенныхпр...   \n",
       "21730   Крометого,вавариимоглипострадатьводительипасса...   \n",
       "194526  Вчислепоследнихназывались«ЮТэйр»,«Сибирь»(S7)и...   \n",
       "32380   ВместоэтогоруководствоЦБсделаловсеправильно»,—...   \n",
       "71417            В2010годуонаотобралаэтозваниеуMicrosoft.   \n",
       "216410  29августа2010годагруппавыступилавмосковскомклу...   \n",
       "126138  Впервоесемействовошлимоделисэкранамив11,6,14и1...   \n",
       "21128   Напервомместе\"Реал\",укоторого78очковиоднаиграв...   \n",
       "205125  Отом,когдавыйдетDragonQuestXдляWiiU,поканесооб...   \n",
       "46819   ВсегоСледственнымдепартаментомМВДзадержано12че...   \n",
       "170623                         Наместоприбылнарядполиции.   \n",
       "119309  Инцидентпроизошел26августавсупермаркетеWalmart...   \n",
       "158759                                       Ялюблютебя!\"   \n",
       "87631                                Мывкакойстранеживем?   \n",
       "32687   Обэтомрассказалпремьер-министрТурцииРеджепТайи...   \n",
       "170164              Судмедэкспертизапризналаеговменяемым.   \n",
       "213412  Педагогадоставиливбольницу,гдеемусделалиоперацию.   \n",
       "190141               ЗащитаобжаловалаприговорвМосгорсуде.   \n",
       "14636   СамыйзначительныйприростзафиксированвАфрике(пл...   \n",
       "225877  Набортунаходилисьактивистыевропейскихправозащи...   \n",
       "\n",
       "                                       space_positions  \n",
       "228066                         [5, 16, 22, 25, 37, 49]  \n",
       "21730              [5, 11, 13, 20, 26, 37, 46, 48, 58]  \n",
       "194526                  [1, 7, 17, 28, 37, 46, 51, 53]  \n",
       "32380              [6, 12, 24, 27, 35, 39, 51, 53, 61]  \n",
       "71417                   [1, 6, 11, 15, 24, 28, 35, 37]  \n",
       "216410             [2, 10, 15, 20, 27, 37, 39, 50, 56]  \n",
       "126138  [1, 8, 18, 24, 31, 33, 42, 44, 50, 53, 55, 60]  \n",
       "21128   [2, 9, 15, 23, 25, 34, 37, 43, 45, 50, 55, 57]  \n",
       "205125  [1, 6, 12, 19, 26, 32, 34, 38, 42, 45, 50, 53]  \n",
       "46819                          [5, 18, 32, 36, 46, 49]  \n",
       "170623                                  [2, 8, 15, 21]  \n",
       "119309                 [8, 18, 21, 29, 31, 44, 52, 54]  \n",
       "158759                                          [1, 7]  \n",
       "87631                                   [2, 4, 10, 17]  \n",
       "32687                       [2, 7, 17, 33, 40, 47, 53]  \n",
       "170164                                    [16, 25, 29]  \n",
       "213412                     [8, 18, 20, 30, 34, 38, 46]  \n",
       "190141                                 [6, 17, 26, 28]  \n",
       "14636                  [5, 18, 26, 39, 41, 48, 54, 59]  \n",
       "225877                          [2, 8, 19, 29, 41, 55]  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90663b2a",
   "metadata": {},
   "source": [
    "Подготовка данных и датасета для BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1db89d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class SpaceDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=MAX_LEN):\n",
    "        self.texts = df['no_space'].tolist()\n",
    "        self.labels = []\n",
    "        for pos_str, txt in zip(df['space_positions'], df['no_space']):\n",
    "            pos = [int(p) for p in pos_str.strip(\"[]\").split(\",\") if p]\n",
    "            label = [0]*len(txt)\n",
    "            for p in pos:\n",
    "                if p < len(label):\n",
    "                    label[p] = 1\n",
    "            if len(label) > max_len:\n",
    "                label = label[:max_len]\n",
    "            self.labels.append(label)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        txt = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        enc = self.tokenizer(txt, truncation=True, max_length=self.max_len, padding='max_length', return_tensors='pt')\n",
    "        input_ids = enc['input_ids'].squeeze(0)\n",
    "        attention_mask = enc['attention_mask'].squeeze(0)\n",
    "        labels = torch.tensor(label[:self.max_len], dtype=torch.long)\n",
    "        if len(labels) < self.max_len:\n",
    "            pad_len = self.max_len - len(labels)\n",
    "            labels = torch.cat([labels, torch.zeros(pad_len, dtype=torch.long)])\n",
    "        return input_ids, attention_mask, labels\n",
    "\n",
    "train_dataset = SpaceDataset(train_df, tokenizer)\n",
    "val_dataset   = SpaceDataset(val_df, tokenizer)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader    = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd97a615",
   "metadata": {},
   "source": [
    "Архитектура и инициализация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6fe66801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fd71ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7078773d",
   "metadata": {},
   "source": [
    "Функции для обучения и валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "12621a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for input_ids, attention_mask, labels in tqdm(loader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        loss = criterion(outputs.view(-1, 2), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            loss = criterion(outputs.view(-1, 2), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            mask = attention_mask.bool()\n",
    "            all_preds.extend(preds[mask].cpu().numpy())\n",
    "            all_labels.extend(labels[mask].cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    p = precision_score(all_labels, all_preds)\n",
    "    r = recall_score(all_labels, all_preds)\n",
    "    return total_loss / len(loader), f1, p, r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc0dea7",
   "metadata": {},
   "source": [
    "Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31350056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2260/2260 [04:14<00:00,  8.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=0.1175, Val Loss=0.0943, F1=0.4697, P=0.8176, R=0.3295\n",
      "Saved best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2260/2260 [04:11<00:00,  8.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss=0.0883, Val Loss=0.0776, F1=0.6048, P=0.8208, R=0.4788\n",
      "Saved best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2260/2260 [04:18<00:00,  8.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss=0.0752, Val Loss=0.0697, F1=0.6978, P=0.7819, R=0.6300\n",
      "Saved best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2260/2260 [04:18<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss=0.0655, Val Loss=0.0628, F1=0.7403, P=0.7977, R=0.6906\n",
      "Saved best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2260/2260 [04:20<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss=0.0579, Val Loss=0.0585, F1=0.7566, P=0.8271, R=0.6972\n",
      "Saved best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2260/2260 [04:23<00:00,  8.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss=0.0521, Val Loss=0.0549, F1=0.7914, P=0.8382, R=0.7496\n",
      "Saved best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2260/2260 [04:23<00:00,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss=0.0474, Val Loss=0.0601, F1=0.7922, P=0.8143, R=0.7713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2260/2260 [04:23<00:00,  8.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss=0.0433, Val Loss=0.0556, F1=0.8097, P=0.8387, R=0.7826\n",
      "Saved best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2260/2260 [04:22<00:00,  8.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss=0.0397, Val Loss=0.0545, F1=0.8250, P=0.8448, R=0.8061\n",
      "Saved best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2260/2260 [04:19<00:00,  8.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss=0.0370, Val Loss=0.0531, F1=0.8315, P=0.8557, R=0.8085\n",
      "Saved best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2260/2260 [04:16<00:00,  8.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss=0.0340, Val Loss=0.0537, F1=0.8430, P=0.8681, R=0.8193\n",
      "Saved best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2260/2260 [04:10<00:00,  9.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss=0.0317, Val Loss=0.0557, F1=0.8396, P=0.8619, R=0.8185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2260/2260 [04:12<00:00,  8.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss=0.0303, Val Loss=0.0573, F1=0.8169, P=0.8416, R=0.7936\n",
      "Learning rate reduced to 0.000015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2260/2260 [04:10<00:00,  9.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss=0.0230, Val Loss=0.0497, F1=0.8643, P=0.8809, R=0.8484\n",
      "Saved best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2260/2260 [04:10<00:00,  9.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss=0.0203, Val Loss=0.0506, F1=0.8673, P=0.8824, R=0.8526\n",
      "Saved best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2260/2260 [04:11<00:00,  9.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss=0.0191, Val Loss=0.0514, F1=0.8626, P=0.8796, R=0.8463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2260/2260 [04:11<00:00,  8.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss=0.0178, Val Loss=0.0574, F1=0.8673, P=0.8779, R=0.8569\n",
      "Learning rate reduced to 0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2260/2260 [04:13<00:00,  8.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss=0.0147, Val Loss=0.0564, F1=0.8720, P=0.8813, R=0.8628\n",
      "Saved best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2260/2260 [04:11<00:00,  8.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss=0.0135, Val Loss=0.0589, F1=0.8718, P=0.8835, R=0.8603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2260/2260 [04:11<00:00,  8.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss=0.0128, Val Loss=0.0616, F1=0.8743, P=0.8821, R=0.8666\n",
      "Saved best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 997/2260 [01:53<02:23,  8.80it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m min_delta = \u001b[32m0.001\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     val_loss, f1, p, r = validate(model, val_loader, criterion, device)\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Train Loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val Loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, F1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, P=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, R=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, optimizer, criterion, device)\u001b[39m\n\u001b[32m     12\u001b[39m     loss.backward()\n\u001b[32m     13\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss / \u001b[38;5;28mlen\u001b[39m(loader)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "best_f1 = 0\n",
    "epochs_no_improve = 0\n",
    "patience = 7\n",
    "min_delta = 0.001\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, f1, p, r = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, F1={f1:.4f}, P={p:.4f}, R={r:.4f}\")\n",
    "\n",
    "    # Проверка на улучшение F1\n",
    "    if f1 - best_f1 > min_delta:\n",
    "        best_f1 = f1\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, \"best_epoch_5.pt\"))\n",
    "        print(\"Saved best checkpoint.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"No improvement for {patience} epochs. Stopping training.\")\n",
    "        break\n",
    "\n",
    "    # уменьшение learning rate, если нет улучшений\n",
    "    if epochs_no_improve > 0 and epochs_no_improve % 2 == 0:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] *= 0.5\n",
    "        print(f\"Learning rate reduced to {optimizer.param_groups[0]['lr']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38debfb",
   "metadata": {},
   "source": [
    "Функция для проверки метрики на тестовом датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_file(model, tokenizer, df, max_len=MAX_LEN):\n",
    "    if 'no_space' not in df.columns:\n",
    "        df['no_space'] = df['tetext_no_spaces'] \n",
    "    if 'space_positions' not in df.columns and 'true_positions' in df.columns:\n",
    "        df['space_positions'] = df['true_positions']\n",
    "\n",
    "    dataset = SpaceDataset(df, tokenizer, max_len=max_len)\n",
    "    loader = DataLoader(dataset, batch_size=16)\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "\n",
    "            mask = attention_mask.bool()\n",
    "            all_preds.extend(preds[mask].cpu().numpy())\n",
    "            all_labels.extend(labels[mask].cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    p = precision_score(all_labels, all_preds)\n",
    "    r = recall_score(all_labels, all_preds)\n",
    "    print(f\"Evaluation: F1={f1:.4f}, Precision={p:.4f}, Recall={r:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "70bd22cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "best_model.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, \"best_epoch_5.pt\"), map_location=device))\n",
    "best_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6f04f12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: F1=0.8725, Precision=0.8794, Recall=0.8657\n"
     ]
    }
   ],
   "source": [
    "evaluate_file(best_model, tokenizer, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7965e3f8",
   "metadata": {},
   "source": [
    "Обработка файла задания от Авито"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48accabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset_1937770_3.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Заменяем первую запятую в каждой строке на точку с запятой\n",
    "corrected_lines = []\n",
    "for line in lines:\n",
    "    # Разделяем строку по первой запятой\n",
    "    parts = line.split(\",\", 1)\n",
    "    if len(parts) == 2:\n",
    "        # Объединяем с разделителем ';'\n",
    "        corrected_line = f\"{parts[0]};{parts[1]}\"\n",
    "        corrected_lines.append(corrected_line)\n",
    "    else:\n",
    "        # Если строка не содержит запятых (например, заголовок), оставляем как есть\n",
    "        corrected_lines.append(line)\n",
    "\n",
    "with open(\"corrected_dataset.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(corrected_lines)\n",
    "\n",
    "task_data = pd.read_csv(\"corrected_dataset.txt\", sep=\";\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3dc90b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_no_spaces</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>куплюайфон14про</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ищудомвПодмосковье</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>сдаюквартирусмебельюитехникой</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>новыйдивандоставканедорого</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>отдамдаромкошку</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1000</td>\n",
       "      <td>Янеусну.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>1001</td>\n",
       "      <td>Весна-яуженегреюпио.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>1002</td>\n",
       "      <td>Весна-скоровырастеттрава.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>1003</td>\n",
       "      <td>Весна-выпосмотрите,каккрасиво.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>1004</td>\n",
       "      <td>Весна-гдемояголова?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1005 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                  text_no_spaces\n",
       "0        0                 куплюайфон14про\n",
       "1        1              ищудомвПодмосковье\n",
       "2        2   сдаюквартирусмебельюитехникой\n",
       "3        3      новыйдивандоставканедорого\n",
       "4        4                 отдамдаромкошку\n",
       "...    ...                             ...\n",
       "1000  1000                        Янеусну.\n",
       "1001  1001            Весна-яуженегреюпио.\n",
       "1002  1002       Весна-скоровырастеттрава.\n",
       "1003  1003  Весна-выпосмотрите,каккрасиво.\n",
       "1004  1004             Весна-гдемояголова?\n",
       "\n",
       "[1005 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17a3e20",
   "metadata": {},
   "source": [
    "Подготовка датасета для инференса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c7027ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128):\n",
    "        self.texts = df['text_no_spaces'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        txt = self.texts[idx]\n",
    "        enc = self.tokenizer(\n",
    "            txt, truncation=True, max_length=self.max_len,\n",
    "            padding='max_length', return_tensors='pt'\n",
    "        )\n",
    "        input_ids = enc['input_ids'].squeeze(0)\n",
    "        attention_mask = enc['attention_mask'].squeeze(0)\n",
    "        return input_ids, attention_mask, txt\n",
    "\n",
    "task_dataset = TaskDataset(task_data, tokenizer, MAX_LEN)\n",
    "task_loader = DataLoader(task_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e9e434",
   "metadata": {},
   "source": [
    "Загрузка сохраненной модели и подготовка ее к инференсу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2803ed7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, \"best_epoch_5.pt\")))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdab06d8",
   "metadata": {},
   "source": [
    "Инференс модели и восстановление текста с пробелами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a70aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions added to task_data and saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "pred_positions = []\n",
    "pred_texts = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, texts in task_loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        preds = torch.argmax(outputs, dim=-1)\n",
    "\n",
    "        for i, txt in enumerate(texts):\n",
    "            mask = attention_mask[i].bool()\n",
    "            pred_labels = preds[i][mask].cpu().numpy()\n",
    "            positions = [j for j, label in enumerate(pred_labels) if label == 1]\n",
    "\n",
    "            # Восстанавливаем текст с пробелами\n",
    "            new_text = \"\"\n",
    "            for idx, char in enumerate(txt):\n",
    "                if idx in positions:\n",
    "                    new_text += \" \"\n",
    "                new_text += char\n",
    "\n",
    "            pred_positions.append(positions)\n",
    "            pred_texts.append(new_text)\n",
    "\n",
    "task_data['predicted_positions'] = pred_positions\n",
    "task_data['predicted_text'] = pred_texts\n",
    "\n",
    "task_data.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Predictions added to task_data and saved to submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d35fed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
